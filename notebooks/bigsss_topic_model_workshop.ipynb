{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling and Attitudes from Twitter Data\n",
    "\n",
    "A short tutorial by **Eduardo Graells-Garrido** / <egraells@dcc.uchile.cl> / [@ZorzalErrante](http://twitter.com/ZorzalErrante) / http://datagramas.cl \n",
    "\n",
    "Last updated: **7/7/2022**\n",
    "\n",
    "Today we have two aims:\n",
    "\n",
    "1. Identify narratives in Twitter discussion with a given context (geographical in this example). We will use topic modelling for this.\n",
    "2. Identify sentiment/emotions in the discussion. We will use transformers (a deep learning architecture) for this.\n",
    "\n",
    "## Preamble\n",
    "\n",
    "This notebook requires the [tsundoku environment](https://github.com/zorzalerrante/tsundoku). Clone the repository and execute the following:\n",
    "\n",
    "```\n",
    "# Create conda environment, install dependencies on it and activate it\n",
    "conda create --name tsundoku --file environment.yml\n",
    "conda activate tsundoku\n",
    "\n",
    "python -m ipykernel install --user --name tsundoku --display-name \"Python (tsundoku)\"\n",
    "```\n",
    "\n",
    "### Google Colab\n",
    "\n",
    "If you use Google Colab you need to install the dependencies in the server. This will take a few minutes! You need to execute the first cell, wait until the server gives you a restart error, and then run the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    !pip uninstall matplotlib -y\n",
    "    !pip install -q condacolab\n",
    "    \n",
    "    import condacolab\n",
    "    condacolab.install_mambaforge()\n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    !git clone https://github.com/zorzalerrante/tsundoku.git tsundoku_git\n",
    "    !mamba env update --name base --file tsundoku_git/environment.yml\n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n",
    "\n",
    "Here we load all the dependencies we will use in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from aves.features.geo import clip_area_geodataframe, to_point_geodataframe\n",
    "from aves.features.sparse import sparse_matrix_to_long_dataframe\n",
    "from aves.features.utils import normalize_rows, standardize_columns\n",
    "from aves.visualization.figures import small_multiples_from_geodataframe\n",
    "from aves.visualization.maps import choropleth_map\n",
    "from aves.visualization.text import draw_wordcloud\n",
    "from scipy.special import softmax\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from tsundoku.features.dtm import build_vocabulary, tokens_to_document_term_matrix\n",
    "from tsundoku.features.text import tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This is a small dataset of tweets about migration in UK. See https://fcorowe.github.io/intro-gds/04-spatial_econometrics.html for a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = dd.read_csv('https://github.com/fcorowe/gds-bigsss-groningen/raw/main/data/uk_geo_tweets_01012019_31012019.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['token'] = tweets['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets['author_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n---\\n'.join(tweets['text'].sample(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.resample('1d', on='created_at').size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary(tweets, 'token')\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "draw_wordcloud(ax, vocab.set_index('token')['frequency'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['frequency'].plot(kind='hist', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(vocab['frequency']).plot(kind='hist', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab = vocab[vocab['frequency'].between(5, vocab['frequency'].quantile(0.985))].reset_index(drop=True)\n",
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab.sort_values('frequency', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "draw_wordcloud(ax, filtered_vocab.set_index('token')['frequency'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['place_name'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geoboundaries.org/index.html#getdata\n",
    "gdf = gpd.read_file('https://raw.githubusercontent.com/wmgeolab/geoBoundaries/793caebea9ccb4bb1c4f38e80684c1166daf288a/releaseData/gbOpen/GBR/ADM2/geoBoundaries-GBR-ADM2-all.zip')\n",
    "gdf['geometry'] = gdf.simplify(0.0001)\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somehow lat and lon are reversed in the original data.\n",
    "\n",
    "tweets = to_point_geodataframe(tweets, 'lat', 'long', drop=True)\n",
    "tweets.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gdf.plot(facecolor='none', edgecolor='black', figsize=(7, 7))\n",
    "tweets.plot(ax=ax, color='purple', markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gpd.sjoin(tweets, gdf, op='within'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = gpd.sjoin(tweets, gdf, op='within')\n",
    "print(len(tweets))\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Potential bug!* How do you know that all tweets are within your geography? If not, the index will have gaps. It would be better to reset it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_counts = (\n",
    "    tweets.groupby(\"shapeName\")\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .rename(\"n_tweets\")\n",
    ")\n",
    "\n",
    "location_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gdf.join(location_counts, on='shapeName', how='inner').plot(column='n_tweets', cmap='PuRd', edgecolor='none', figsize=(12, 12))\n",
    "gdf.plot(facecolor='none', edgecolor='black', linewidth=0.1, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narratives\n",
    "\n",
    "### Main Representation: Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tokens_to_document_term_matrix(tweets, 'tweet_id', 'token', filtered_vocab['token'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that the most frequent words are not necessarily the most informative. We filtered out some of them, but that only diminishes the problem.\n",
    "\n",
    "One way of improving the situation is to assign a weight to each word.\n",
    "\n",
    "The most common weighting formula is TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "\n",
    "dtm_weighted = tfidf.fit_transform(dtm)\n",
    "dtm_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_importances = sparse_matrix_to_long_dataframe(dtm_weighted, var_map=filtered_vocab['token'].to_dict())\n",
    "word_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_word_importance = (\n",
    "    word_importances.groupby(\"column\")[\"value\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "global_word_importance.head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_tweet_idx = (\n",
    "    tweets.groupby(\"shapeName\")\n",
    "    .apply(lambda x: x.index.values)\n",
    "    #.loc[location_counts.index]\n",
    ")\n",
    "\n",
    "place_tweet_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_dtm = np.vstack(place_tweet_idx.map(lambda x: np.squeeze(np.array(dtm[x].sum(axis=0)))))\n",
    "place_dtm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_words = pd.DataFrame(\n",
    "    tfidf.transform(place_dtm).todense(),\n",
    "    index=place_tweet_idx.index,\n",
    "    columns=filtered_vocab[\"token\"],\n",
    ")\n",
    "\n",
    "place_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_words.T.apply(lambda x: x.sort_values(ascending=False).head(10).index).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model: Non-Negative Matrix Factorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=20, random_state=42)\n",
    "nmf_document_topic = nmf_model.fit_transform(dtm_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_term_topic = nmf_model.components_\n",
    "nmf_term_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_term_topic = pd.DataFrame(nmf_term_topic.T, index=filtered_vocab['token']).pipe(normalize_rows)\n",
    "nmf_term_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(nmf_term_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_term_topic.apply(lambda x: x.sort_values(ascending=False).head(25).index).add_prefix('topic_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_place_topic = nmf_model.transform(tfidf.transform(place_dtm))\n",
    "nmf_place_topic = pd.DataFrame(nmf_place_topic, index=place_tweet_idx.index).add_prefix('topic_').pipe(normalize_rows)\n",
    "nmf_place_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(nmf_place_topic, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic_labels = nmf_term_topic.apply(lambda x: '\\n'.join(x.sort_values(ascending=False).head(15).index))\n",
    "nmf_topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = small_multiples_from_geodataframe(gdf, n_variables=len(nmf_place_topic.columns), height=7, col_wrap=5)\n",
    "\n",
    "place_topic = nmf_place_topic\n",
    "topic_labels = nmf_topic_labels\n",
    "\n",
    "joint_gdf = gdf.join(place_topic, on='shapeName')\n",
    "\n",
    "for ax, col, labels in zip(axes, place_topic.columns, topic_labels.values):\n",
    "    gdf.plot(facecolor='none', edgecolor='#abacab', linewidth=0.5, ax=ax, aspect=None)\n",
    "    \n",
    "    choropleth_map(ax, joint_gdf[joint_gdf[col] >= 0.05], col, edgecolor='black', linewidth=0.5, k=5, edgebinning=\"fisher_jenks\", palette='RdPu',\n",
    "        cbar_args=dict(\n",
    "            label=f\"{col}\",\n",
    "            height=\"25%\",\n",
    "            width=\"3%\",\n",
    "            orientation=\"vertical\",\n",
    "            location=\"lower left\",\n",
    "            label_size=\"small\",\n",
    "            bbox_to_anchor=(0.0, 0.0, 0.8, 0.95),\n",
    "        ),)\n",
    "    ax.set_title(col)\n",
    "\n",
    "    ax.annotate(labels, (0.99, 0.99), xycoords='axes fraction', ha='right', va='top', fontsize='medium')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=20, random_state=42)\n",
    "lda_document_topic = lda_model.fit_transform(dtm)\n",
    "lda_term_topic = pd.DataFrame(lda_model.components_.T, index=filtered_vocab['token'])\n",
    "lda_term_topic.apply(lambda x: x.sort_values(ascending=False).head(25).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_place_topic = lda_model.transform(place_dtm)\n",
    "lda_place_topic = pd.DataFrame(lda_place_topic, index=place_tweet_idx.index).add_prefix('topic_')\n",
    "lda_place_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(lda_place_topic, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_labels = lda_term_topic.apply(lambda x: '\\n'.join(x.sort_values(ascending=False).head(15).index))\n",
    "lda_topic_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = small_multiples_from_geodataframe(gdf, n_variables=len(lda_place_topic.columns), height=7, col_wrap=5)\n",
    "\n",
    "place_topic = lda_place_topic\n",
    "topic_labels = lda_topic_labels\n",
    "\n",
    "joint_gdf = gdf.join(place_topic, on='shapeName')\n",
    "\n",
    "for ax, col, labels in zip(axes, place_topic.columns, topic_labels.values):\n",
    "    gdf.plot(facecolor='none', edgecolor='#abacab', linewidth=0.5, ax=ax, aspect=None)\n",
    "    \n",
    "    choropleth_map(ax, joint_gdf[joint_gdf[col] >= 0.05], col, edgecolor='black', linewidth=0.5, k=5, edgebinning=\"fisher_jenks\", palette='RdPu',\n",
    "        cbar_args=dict(\n",
    "            label=f\"{col}\",\n",
    "            height=\"25%\",\n",
    "            width=\"3%\",\n",
    "            orientation=\"vertical\",\n",
    "            location=\"lower left\",\n",
    "            label_size=\"small\",\n",
    "            bbox_to_anchor=(0.0, 0.0, 0.8, 0.95),\n",
    "        ),)\n",
    "    ax.set_title(col)\n",
    "\n",
    "    ax.annotate(labels, (0.99, 0.99), xycoords='axes fraction', ha='right', va='top', fontsize='medium')\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one is better? We can't say. It will depend on your task :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment using Transformers\n",
    "\n",
    "Transformers are a deep learning architecture that has been popularized due to their availability and the ability to fine-tune.\n",
    "\n",
    "Fine-tuning means that you can download a model and re-train it for your specific task, taking advantage of all previous structure already inferred by the model.\n",
    "\n",
    "Fortunately, the Huggingface transformers library makes it very easy to download models and put them into operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "task='emotion'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return pd.Series(scores, index=labels)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_emotion(\"Good night ðŸ˜Š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets= tweets[['text']].sample(5)\n",
    "sample_tweets.join(sample_tweets['text'].apply(predict_emotion)).set_index('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_emotions = tweets['text'].apply(predict_emotion)\n",
    "tweet_emotions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_sentiment = (\n",
    "    tweets.join(tweet_emotions)\n",
    "    .groupby(\"shapeName\")[tweet_emotions.columns]\n",
    "    .median()\n",
    "    #.pipe(normalize_rows)\n",
    ")\n",
    "place_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_sentiment.sort_values('anger', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = small_multiples_from_geodataframe(\n",
    "    gdf, n_variables=len(labels), height=7, col_wrap=4, remove_axes=True\n",
    ")\n",
    "\n",
    "joint_gdf = gdf.join(place_sentiment.pipe(standardize_columns), on=\"shapeName\")\n",
    "\n",
    "for ax, col in zip(axes, place_sentiment.columns):\n",
    "    choropleth_map(\n",
    "        ax,\n",
    "        joint_gdf,\n",
    "        col,\n",
    "        k=5,\n",
    "        linewidth=0.5,\n",
    "        edgecolor=\"black\",\n",
    "        binning=\"fisher_jenks\",\n",
    "        cbar_args=dict(\n",
    "            label=f\"{col} [z]\",\n",
    "            height=\"25%\",\n",
    "            width=\"3%\",\n",
    "            orientation=\"vertical\",\n",
    "            location=\"upper right\",\n",
    "            label_size=\"small\",\n",
    "            bbox_to_anchor=(0.0, 0.0, 0.8, 0.95),\n",
    "        ),\n",
    "    )\n",
    "    # joint_gdf.plot(ax=ax, column=col, aspect=None, cmap='RdBu')\n",
    "    # gdf.plot(facecolor='none', edgecolor='black', linewidth=0.1, ax=ax, aspect=None)\n",
    "    ax.set_title(col)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.subplots_adjust(hspace=0.001, wspace=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://woeplanet.org/id/23416974/\n",
    "london_bbox = [-0.51035, 51.286839, 0.33403, 51.692322]\n",
    "gdf_london = clip_area_geodataframe(gdf, london_bbox, buffer=0.01)\n",
    "gdf_london.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = small_multiples_from_geodataframe(gdf_london, n_variables=len(labels), height=6, col_wrap=5, remove_axes=False)\n",
    "\n",
    "joint_gdf = gdf.join(place_sentiment.pipe(standardize_columns), on='shapeName')\n",
    "\n",
    "for ax, col in zip(axes, place_sentiment.columns):\n",
    "    choropleth_map(\n",
    "        ax,\n",
    "        joint_gdf,\n",
    "        col,\n",
    "        k=5,\n",
    "        linewidth=0.5,\n",
    "        edgecolor=\"black\",\n",
    "        binning=\"fisher_jenks\",\n",
    "        legend=None\n",
    "    )\n",
    "    #joint_gdf.plot(ax=ax, column=col, aspect=None, cmap='RdBu')\n",
    "    #gdf.plot(facecolor='none', edgecolor='black', linewidth=0.1, ax=ax, aspect=None)\n",
    "    ax.set_title(col)\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_ratio = (london_bbox[2] - london_bbox[0]) / (london_bbox[3] - london_bbox[1])\n",
    "aspect_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = small_multiples_from_geodataframe(\n",
    "    gdf, n_variables=4, height=9, col_wrap=4, remove_axes=True\n",
    ")\n",
    "\n",
    "joint_gdf = gdf.join(place_sentiment.pipe(standardize_columns), on=\"shapeName\")\n",
    "\n",
    "for ax, col in zip(axes, place_sentiment.columns):\n",
    "    gdf.plot(facecolor='none', edgecolor='#abacab', linewidth=0.5, ax=ax, aspect=None)\n",
    "    \n",
    "    choropleth_map(\n",
    "        ax,\n",
    "        joint_gdf,\n",
    "        col,\n",
    "        k=5,\n",
    "        linewidth=0.5,\n",
    "        edgecolor=\"black\",\n",
    "        binning=\"fisher_jenks\",\n",
    "        cbar_args=dict(\n",
    "            label=f\"{col} [z]\",\n",
    "            height=\"25%\",\n",
    "            width=\"3%\",\n",
    "            orientation=\"vertical\",\n",
    "            location=\"upper right\",\n",
    "            label_size=\"small\",\n",
    "            bbox_to_anchor=(0.0, 0.0, 0.8, 0.95),\n",
    "        ),\n",
    "    )\n",
    "    # joint_gdf.plot(ax=ax, column=col, aspect=None, cmap='RdBu')\n",
    "    \n",
    "    ax.set_title(col)\n",
    "\n",
    "    axins = ax.inset_axes([0.75, -0.12, 0.4, 0.4 / aspect_ratio])\n",
    "    axins.set_axis_off()\n",
    "    #axins.imshow(Z2, extent=extent, origin=\"lower\")\n",
    "    # sub region of the original image\n",
    "    #x1, x2, y1, y2 = -1.5, -0.9, -2.5, -1.9\n",
    "    axins.set_xlim(london_bbox[0], london_bbox[2])\n",
    "    axins.set_ylim(london_bbox[1], london_bbox[3])\n",
    "    #axins.set_xticklabels([])\n",
    "    #axins.set_yticklabels([])\n",
    "\n",
    "    choropleth_map(\n",
    "        axins,\n",
    "        joint_gdf,\n",
    "        col,\n",
    "        k=5,\n",
    "        linewidth=0.2,\n",
    "        edgecolor=\"black\",\n",
    "        binning=\"fisher_jenks\",\n",
    "        legend=None\n",
    "    )\n",
    "\n",
    "    ax.indicate_inset_zoom(axins, edgecolor=\"black\", zorder=50)\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlating Emotion and Narratives\n",
    "\n",
    "We may want to characterize the topics underpinning the discussion. For instance, we cannot say with confidence that a topic characterized by a high association to a negative word is negative, because we don't know the context of the negative word. However, the sentiment characterization does that. \n",
    "\n",
    "Since we have estimated these measures for the same unit of analysis, one step toward characterizing topics is through correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic_x_emotion = (\n",
    "    place_sentiment.join(nmf_place_topic)\n",
    "    .fillna(0)\n",
    "    .corr()\n",
    "    .loc[nmf_place_topic.columns, place_sentiment.columns]\n",
    "    .set_index(nmf_topic_labels.map(lambda x: x.replace(\"\\n\", \",\")))\n",
    ")\n",
    "\n",
    "lda_topic_x_emotion = (\n",
    "    place_sentiment.join(lda_place_topic)\n",
    "    .fillna(0)\n",
    "    .corr()\n",
    "    .loc[lda_place_topic.columns, place_sentiment.columns]\n",
    "    .set_index(lda_topic_labels.map(lambda x: x.replace(\"\\n\", \",\")))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(nmf_topic_x_emotion, center=0, figsize=(16, 9), annot=True, metric='cosine')\n",
    "g.fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(lda_topic_x_emotion, center=0, figsize=(16, 9), annot=True, metric='cosine')\n",
    "g.fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one to pick? It seems that the results are not _that_ different. \n",
    "\n",
    "This is not the end of the study. We should do a careful qualitative analysis that can be supported by these numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining Questions\n",
    "\n",
    "### How to select a topic model?\n",
    "\n",
    "My advice would be to test if simpler models give you reasonable results. If so, before moving to a more complex model, see if you can improve your data or your pre-processing. I like NMF for its simplicity and speed. And results are usually good enough. As with LDA, you can find evidence regarding its usefulness, thus, it is not a choice hard to justify.\n",
    "\n",
    "Note that there are multiple versions of NMF and LDA. The [gensim](https://radimrehurek.com/gensim/) library is a good starting point as it has many implementations of those variants, as well as of other models.\n",
    "\n",
    "### How to select the number of topics?\n",
    "\n",
    "Do not focus only on quantitative measurements. Think about your assumptions of the data. Keep in mind typical evaluations (such as selecting a model based on Log-Likelihood or similar), but remember that those metrics are not necessarily related to your needs or assumptions about the data and the phenomena under study.\n",
    "\n",
    "For instance, NMF tries to reconstruct the original matrix. As such, the \"goodness of fit\" is measured through matrix reconstruction error. You will notice that, as you increase the rank of the latent matrices, the fit improves always. \n",
    "\n",
    "Note that topics are _latent_, sometimes they do not have a human interpretation. A way to surpass this and have more interpretable topics is to use semi-supervised models. One of them is [Corex Topic Model](https://github.com/gregversteeg/corex_topic). where you can anchor words to topics as a way to guide the inference.\n",
    "\n",
    "Always visualize what you do :) It will help you to pinpoint potential insights and also potential errors.\n",
    "\n",
    "### Which transformer model to use?\n",
    "\n",
    "I would say that every week there is a new model! The field is growing in a quite spectacular way, what I suggest is to find model authors that you trust and that have evaluated the new models in datasets similar to yours. For instance, here we used [tweeteval](https://github.com/cardiffnlp/tweeteval), which is trained and fine-tuned with tweets.\n",
    "\n",
    "## Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tsundoku)",
   "language": "python",
   "name": "tsundoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
